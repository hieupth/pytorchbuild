context:
  name: pytorch
  rev: main
  version: nightly
  python: 3.12
  gcc: 13.3
  cuda: 12.8
  cudnn: 9.8
  archs: 8.6;9.0;10.0;12.0

package:
  name: ${{name|lower}}
  version: ${{version}}

source:
  git: https://github.com/pytorch/pytorch
  rev: ${{rev}}

build:
  script:
    env:
      USE_CUDA: 1
      USE_MAGMA: 1
      USE_SYSTEM_NCCL: 1
      CONDA_PREFIX: $PREFIX
      BUILD_CUSTOM_PROTOBUF: 0
      TORCH_CUDA_ARCH_LIST: "${{archs}}"
      NCCL_ROOT: $PREFIX:$PREFIX/targets/x86_64-linux:$NCCL_ROOT
      CUDA_PATH: $PREFIX:$PREFIX/targets/x86_64-linux:$CUDA_PATH
      CUDA_HOME: $PREFIX:$PREFIX/targets/x86_64-linux:$CUDA_HOME
      CUDA_BIN_PATH: $PREFIX:$PREFIX/targets/x86_64-linux:$CUDA_BIN_PATH
      CUDA_TOOLKIT_ROOT: $PREFIX:$PREFIX/targets/x86_64-linux:$CUDA_TOOLKIT_ROOT
    content: |
      > third_party/NNPACK/cmake/DownloadSix.cmake
      bash .ci/docker/common/install_magma_conda.sh "${{cuda}}"
      $PYTHON setup.py install \
        --prefix=$PREFIX

requirements:
  build:
    - gcc=${{gcc}}
    - gxx=${{gcc}}
    - make
    - ninja
    - mkl-static
    - mkl-include
  host:
    - python=3.12
    - setuptools
    - pip
    - six
    - curl
    - pybind11
    - cmake<4.0
    - pyyaml
    - protobuf
    - typing-extensions
    - nccl
    - eigen
    - sleef
    - onednn
    - sccache
    - numpy>2.0
    - libabseil
    - libcudss-dev
    - cusparselt-dev
    - cudnn=${{cudnn}}
    - cuda-cccl=${{cuda}}
    - cuda-nvcc=${{cuda}}
    - cuda-nvtx=${{cuda}}
    - cuda-nvrtc=${{cuda}}
    - cuda-toolkit=${{cuda}}
    - cuda-compiler=${{cuda}}
  run:
    - python=3.12
    - mkl
    - numpy
    - pyyaml
    - libnuma
    - typing-extensions
    - cudnn=${{cudnn}}
    - cuda-toolkit=${{cuda}}

tests:
  - script:
    - python -c "import torch; print(torch.cuda.is_available()); print(torch.cuda.get_device_name(0));"